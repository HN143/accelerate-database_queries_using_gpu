{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Deduplicated CSV File\n",
    "This notebook processes the input CSV file, removes duplicate `query_id` entries, and creates a new CSV file with averaged values for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "# input_path = \"tpc-ds/result_log/result_log_20GB/time_3/query_sys_params.csv\"\n",
    "# average_path = \"tpc-ds/result_log/result_log_20GB/time_3/average_query_sys_params.csv\"\n",
    "# worst_path = \"tpc-ds/result_log/result_log_20GB/time_3/worst_query_sys_params.csv\"\n",
    "# query_times_path = \"tpc-ds/result_log/result_log_20GB/time_3/query_times.csv\"\n",
    "input_path = \"tpc-h/result_log/result_log_20GB/time_3/query_sys_params.csv\"\n",
    "average_path = \"tpc-h/result_log/result_log_20GB/time_3/average_query_sys_params.csv\"\n",
    "worst_path = \"tpc-h/result_log/result_log_20GB/time_3/worst_query_sys_params.csv\"\n",
    "query_times_path = \"tpc-h/result_log/result_log_20GB/time_3/query_times.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['query_id', 'cpu_used(%)', 'ram_used(gb)']\n",
      "Average data:\n",
      "   query_id  cpu_used(%)  ram_used(gb)\n",
      "0         1        88.62          1.22\n",
      "1         2        46.70          1.04\n",
      "2         3        78.99          1.41\n",
      "3         4        79.00          1.30\n",
      "4         5        74.58          1.47\n",
      "\n",
      "Worst case data:\n",
      "   query_id  cpu_used(%)  ram_used(gb)\n",
      "0         1         90.9          1.73\n",
      "1         2         76.2          1.05\n",
      "2         3         88.1          1.85\n",
      "3         4         89.4          1.63\n",
      "4         5         89.2          2.06\n"
     ]
    }
   ],
   "source": [
    "# Read the input CSV file - don't skip the first row since it contains headers\n",
    "data = pd.read_csv(input_path)\n",
    "\n",
    "# Clean column names (remove spaces)\n",
    "data.columns = [col.strip() for col in data.columns]\n",
    "\n",
    "# Print column names to see what's available\n",
    "print(\"Available columns:\", data.columns.tolist())\n",
    "\n",
    "# Convert columns to numeric types\n",
    "data['cpu_used(%)'] = pd.to_numeric(data['cpu_used(%)'], errors='coerce')\n",
    "data['ram_used(gb)'] = pd.to_numeric(data['ram_used(gb)'], errors='coerce')\n",
    "\n",
    "# Now group by query_id and calculate the average for duplicates\n",
    "average_data = data.groupby('query_id', as_index=False).agg({\n",
    "    'cpu_used(%)': 'mean',\n",
    "    'ram_used(gb)': 'mean'\n",
    "})\n",
    "\n",
    "# Calculate worst-case (maximum) values\n",
    "worst_data = data.groupby('query_id', as_index=False).agg({\n",
    "    'cpu_used(%)': 'max',\n",
    "    'ram_used(gb)': 'max'\n",
    "})\n",
    "\n",
    "# Round to 2 decimal places\n",
    "average_data = average_data.round(2)\n",
    "worst_data = worst_data.round(2)\n",
    "\n",
    "# Print the first few rows of each to verify\n",
    "print(\"Average data:\")\n",
    "print(average_data.head())\n",
    "print(\"\\nWorst case data:\")\n",
    "print(worst_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the query times data\n",
    "\n",
    "query_times = pd.read_csv(query_times_path)\n",
    "\n",
    "# Clean column names\n",
    "query_times.columns = [col.strip() for col in query_times.columns]\n",
    "\n",
    "# Convert real_time from seconds to milliseconds\n",
    "query_times['time(ms)'] = query_times['real_time(s)'] * 1000\n",
    "\n",
    "# Group by query_id and calculate the average for duplicates in query_times\n",
    "deduplicated_times = query_times.groupby('query_id', as_index=False).agg({\n",
    "    'real_time(s)': 'mean',\n",
    "    'user_time(s)': 'mean',\n",
    "    'sys_time(s)': 'mean',\n",
    "    'time(ms)': 'mean'\n",
    "})\n",
    "\n",
    "# Merge the deduplicated system parameters with the deduplicated times\n",
    "merged_average_data = pd.merge(average_data, deduplicated_times[['query_id', 'time(ms)']], \n",
    "                      on='query_id', how='left')\n",
    "worst_average_data = pd.merge(worst_data, deduplicated_times[['query_id', 'time(ms)']], \n",
    "                      on='query_id', how='left')\n",
    "# Save the merged data to a new CSV file\n",
    "merged_average_data.to_csv(average_path, index=False)\n",
    "worst_average_data.to_csv(worst_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
